# 무료로 구축하는 한국 검색 트렌드 분석 대시보드

2만원 예산 제약 하에서도 **네이버 데이터랩 API(완전 무료, 일 1,000회)와 Google Gemini API(무료 티어), Streamlit Community Cloud(무료 호스팅)**를 결합하면 프로덕션급 트렌드 분석 대시보드를 완전 무료로 구축할 수 있다. 이 조합은 월 3만 개 이상의 키워드 데이터를 수집하고, AI 기반 키워드 확장을 수행하며, 연간 히트맵 달력으로 시각화하는 완전한 시스템을 제공한다. 가장 중요한 것은 모든 핵심 컴포넌트가 한국 시장에 최적화되어 있으며, 네이버가 국내 검색 시장의 60% 이상을 차지하는 환경에서 Google Trends보다 더 정확한 데이터를 제공한다는 점이다.

이 가이드는 실제 구현 가능한 코드, 단계별 설정 방법, 그리고 F&B 프랜차이즈 이벤트 크롤링까지 포함한 완전한 솔루션을 제시한다. 네이버 데이터랩은 2016년부터의 검색 트렌드 데이터를 제공하며, API 키 발급이 5분 내에 완료되고, 별도 비용이나 심사 없이 즉시 사용할 수 있다. 프로젝트 전체를 GitHub Actions 무료 크론잡으로 자동화하고, DuckDB로 로컬에서 빠르게 데이터를 처리하며, 최종적으로 Streamlit으로 항상 접속 가능한 대시보드를 배포할 수 있다.

## 완전 무료 핵심 데이터 소스 구성

네이버 데이터랩 API는 이 프로젝트의 중심축이다. **developers.naver.com에서 계정 생성 후 애플리케이션 등록 시 '데이터랩(검색어 트렌드)' API를 선택**하면 Client ID와 Client Secret을 즉시 받을 수 있다. 신용카드 등록이나 유료 플랜 가입 없이 완전 무료로 제공되며, 일일 1,000회 호출 한도는 여러 계정을 통해 확장 가능하다. API는 2016년 1월 1일부터 현재까지의 데이터를 제공하며, 일/주/월 단위 시계열 데이터와 함께 기기별(PC/모바일), 성별, 연령대별 필터링을 지원한다.

PyNaver 라이브러리를 사용하면 구현이 매우 간단하다. `pip install PyNaver` 후 5줄의 코드로 데이터를 수집할 수 있다. 중요한 점은 네이버 데이터랩이 반환하는 값이 절대 검색량이 아닌 상대 비율(0-100)이라는 것이다. 최대 검색량을 100으로 정규화하고 나머지를 비례 계산한 값이므로, 여러 키워드를 비교할 때는 항상 기준 키워드를 포함해야 정확한 비교가 가능하다. API는 한 번에 최대 5개 키워드 그룹을 쿼리할 수 있으며, 각 그룹당 최대 20개 키워드를 포함할 수 있다.

일일 1,000회 제한을 우회하는 가장 효과적인 전략은 **여러 네이버 계정으로 애플리케이션을 등록하고 API 키를 로테이션**하는 것이다. 5개 계정을 사용하면 일일 5,000회로 확장되며, 이는 월 15만 회 호출을 의미한다. 동시에 SQLite나 DuckDB를 사용한 적극적인 캐싱으로 동일한 쿼리를 반복하지 않고, 월간 단위 집계를 활용하면 API 호출을 최소화할 수 있다. 네이버 클라우드 플랫폼 콘솔에서 실시간 사용량을 모니터링하고, 일일 한도의 80%에 도달하면 다음 키를 사용하도록 자동 전환하는 로직을 구현해야 한다.

## Google Trends 대안과 프록시 전략의 현실

pytrends 라이브러리는 **2025년 4월 17일 공식적으로 아카이브되어 더 이상 유지보수되지 않는다**. 최종 버전 4.9.1은 여전히 작동하지만, Rate Limit 429 에러가 빈번하게 발생하며 프로덕션 환경에서는 권장되지 않는다. Google Trends는 공식 API를 제공하지 않으므로, pytrends는 웹 스크래핑 방식으로 데이터를 수집하는데, Google이 이를 감지하고 차단하는 경우가 많다. 연구 결과에 따르면 약 1,400회 연속 요청 후 4시간 동안 차단되며, 60초 대기 후 재시도가 필요하다.

프록시 로테이션이 이론적 해결책으로 제시되지만 실제로는 여러 문제가 있다. 무료 프록시는 신뢰할 수 없고 느리며 보안 위험이 있다. 유료 프록시 서비스(Bright Data, Smartproxy 등)는 월 75-500달러로 2만원 예산을 초과한다. 더 현실적인 대안은 **SerpApi의 Google Trends API**인데, 월 100회 무료 검색을 제공하고 이후 유료이지만 안정적이고 429 에러가 없다. 한국 시장(`geo='KR'`) 지원이 완벽하며 응답 속도가 1-2초로 빠르다.

예산 제약을 고려할 때 가장 현명한 선택은 **네이버 데이터랩을 주요 데이터 소스로 사용하고, Google Trends는 보조 지표로만 제한적으로 활용**하는 것이다. pytrends로 일 10-20회 정도의 핵심 키워드만 수집하고 60초 간격을 두면 차단을 피할 수 있다. 또는 SerpApi 무료 티어의 월 100회를 전략적으로 사용하여 글로벌 트렌드와 한국 트렌드를 비교하는 용도로 활용할 수 있다. 네이버가 한국 검색 시장의 60% 이상을 차지하므로, 국내 기획전 타이밍 분석에는 네이버 데이터가 실제로 더 가치 있다.

## 완전 무료 대시보드 구축과 배포

Streamlit Community Cloud는 이 프로젝트에 가장 적합한 선택이다. **1GB RAM, 무제한 공개 앱, 항상 켜져 있는 호스팅**을 완전 무료로 제공하며, GitHub 저장소와 연결하면 2-5분 내에 자동 배포된다. 콜드 스타트가 없어 사용자 경험이 우수하고, Python으로 순수하게 작성할 수 있어 HTML/CSS/JavaScript 지식이 필요 없다. Gradio는 16GB RAM으로 더 넉넉하지만 약 48시간 비활성화 후 슬립 상태가 되어 최초 접속 시 30-60초 대기가 필요하다.

Streamlit 앱은 놀라울 정도로 간단하다. `st.title()`, `st.line_chart()`, `st.date_input()` 같은 직관적인 함수로 대시보드를 구성하고, 데이터가 변경되면 자동으로 UI가 업데이트된다. 한글 텍스트 표시는 완벽하게 지원되며, Plotly 차트를 `st.plotly_chart()`로 임베딩하면 인터랙티브한 시각화를 구현할 수 있다. 다만 날짜 선택기 같은 UI 요소는 영어로 표시되므로, 한글 레이블을 별도로 추가해야 한다.

배포 프로세스는 다음과 같다. 로컬에서 `streamlit run app.py`로 개발하고, GitHub 저장소에 `requirements.txt`와 함께 푸시한다. streamlit.io/cloud에서 GitHub 계정으로 로그인하고 "New app"을 클릭하여 저장소와 메인 파일을 지정하면 자동으로 빌드되고 배포된다. 이후 GitHub에 푸시할 때마다 자동으로 재배포되므로, 지속적인 업데이트가 가능하다. 1GB RAM 제한은 검색 트렌드 시각화에는 충분하지만, 대용량 머신러닝 모델을 로드하려면 최적화가 필요하다.

## LLM 기반 키워드 확장 무료 솔루션

Google Gemini API의 무료 티어는 키워드 확장에 완벽한 선택이다. **Gemini 2.5 Flash 모델은 분당 15회, 일일 1,500회 요청을 무료로 제공**하며, 컨텍스트 윈도우가 100만 토큰으로 매우 크다. 한국어 지원이 공식적으로 명시되어 있고, 상업적 사용이 허용되며, 신용카드 등록 없이 API 키를 발급받을 수 있다. 1,000개 키워드 확장 요청(약 550K 토큰)을 처리해도 완전 무료 범위 내에 있다.

로컬 대안으로 Ollama를 사용하면 API 비용 없이 무제한 사용이 가능하다. **EXAONE 3.5(32B) 모델은 LG AI Research에서 개발한 한국어/영어 이중 언어 모델**로, 한국어 키워드 생성에 최적화되어 있다. 설치는 `curl -fsSL https://ollama.com/install.sh | sh`로 간단하고, `ollama pull exaone3.5:32b`로 모델을 다운로드한 후 Python에서 `import ollama`로 사용한다. 하드웨어 요구사항은 32B 모델의 경우 32GB RAM이 권장되지만, 7B 모델(Qwen2.5)은 8GB RAM으로도 충분하다.

실제 구현 시 두 가지 옵션을 병행할 수 있다. 빠른 응답이 필요한 실시간 키워드 확장은 Gemini API를 사용하고(응답 2초 이내), 대량 배치 처리나 프라이버시가 중요한 경우는 Ollama를 사용하는 전략이다. Gemini API 호출 예제는 간단하다. `import google.generativeai as genai`로 라이브러리를 임포트하고, API 키를 설정한 후 `model.generate_content(prompt)` 한 줄로 결과를 받을 수 있다. 프롬프트에 "당신은 한국 SEO 전문가입니다"라고 명시하면 한국어 출력 품질이 크게 향상된다.

키워드 확장 프롬프트는 구체적이어야 한다. 시드 키워드, 타겟 개수, 검색 의도 분류(정보형/탐색형/거래형), 롱테일 키워드 포함 요청, 그리고 JSON 형식 출력 지정을 포함해야 한다. 예를 들어 "커피"라는 시드 키워드에서 "원두 커피 추천", "홈카페 커피 레시피", "스타벅스 신메뉴", "커피 머신 비교" 같은 다양한 의도의 키워드를 생성할 수 있다. LLM은 계절성 키워드(여름 아이스커피, 겨울 따뜻한 음료)와 지역 기반 키워드(강남 카페, 부산 커피 맛집)도 제안할 수 있어, 수동으로는 발견하기 어려운 인사이트를 제공한다.

## 연간 히트맵 달력 시각화 기술

plotly-calplot은 **GitHub 스타일의 인터랙티브 캘린더 히트맵**을 생성하는 최고의 선택이다. `pip install plotly-calplot` 후 단 5줄의 코드로 365일 데이터를 시각화할 수 있으며, Plotly의 모든 인터랙티브 기능(줌, 호버 툴팁, 필터링)을 지원한다. 다크 테마와 다양한 컬러 스케일을 제공하며, Streamlit에 `st.plotly_chart(fig)`로 바로 임베딩할 수 있다. 데이터프레임은 'date'와 'value' 두 컬럼만 있으면 되므로, 네이버 API 응답을 간단히 변환하여 사용할 수 있다.

대안으로 calmap(matplotlib 기반)은 정적 이미지를 생성하며, 논문이나 보고서에 포함하기 적합하다. dayplot은 2025년 새로 출시된 라이브러리로, 여러 연도를 서브플롯으로 배치하는 기능이 뛰어나다. JavaScript 옵션으로는 Cal-Heatmap이 있는데, D3.js 기반으로 매우 커스터마이징 가능하며 MIT 라이선스로 완전 무료다. 연/월/주/일/시/분 단위의 다양한 시간 세분성을 지원하고, 플러그인으로 범례와 툴팁을 추가할 수 있다.

연간 뷰 모범 사례로는 **12개월을 수평으로 배치하고, 각 셀 크기를 10-15px**로 설정하며, 셀 간 간격을 2-4px로 유지하는 것이 좋다. 컬러 스케일은 4-5단계 강도 레벨이 적당하며, 너무 많은 단계는 오히려 구분을 어렵게 만든다. 호버 툴팁에는 정확한 날짜, 검색량 값, 그리고 전주 대비 증감률을 표시하면 유용하다. 클릭 이벤트를 연결하여 특정 날짜를 클릭하면 상세 데이터를 보여주는 드릴다운 기능을 구현할 수 있다.

한국 공휴일과 방학 데이터를 히트맵에 표시하면 패턴 분석이 훨씬 명확해진다. Python holidays 라이브러리는 `holidays.KR()`로 한국 공휴일을 완벽 지원하며, 설날과 추석 같은 음력 공휴일도 자동 계산한다. 히트맵 셀에 공휴일 표시를 위해 다른 색상의 테두리를 추가하거나, 툴팁에 "설날" 같은 공휴일 이름을 표시할 수 있다. 방학 기간은 data.go.kr에서 교육부 학사일정 데이터를 가져와 유사하게 표시하면 된다.

## 자동화된 데이터 수집 파이프라인

GitHub Actions 무료 티어는 **퍼블릭 저장소에 무제한 실행 시간**을 제공하여 완벽한 무료 크론잡 솔루션이다. 프라이빗 저장소도 월 2,000분을 무료로 제공하며, Linux 러너는 1배 소비 계수(Windows 2배, macOS 10배)이므로 효율적이다. 워크플로우는 `.github/workflows/collect-data.yml` 파일에 정의하며, `cron: '10 0 * * *'`로 매일 자동 실행을 설정할 수 있다. 중요한 점은 모든 시간이 UTC 기준이므로 한국 시간(UTC+9)으로 변환해야 한다는 것이다.

워크플로우는 저장소를 체크아웃하고, Python 환경을 설정하고, 의존성을 설치한 후 데이터 수집 스크립트를 실행한다. API 키 같은 민감 정보는 GitHub Secrets에 저장하고 `${{ secrets.API_KEY }}`로 참조한다. 수집된 데이터는 `data/` 디렉토리에 저장하고, git add/commit/push로 저장소에 자동 커밋한다. 실패 시 Slack이나 Discord 웹훅으로 알림을 보내도록 설정하면 문제를 즉시 감지할 수 있다.

로컬 대안으로 Python schedule 라이브러리는 크로스 플랫폼 스케줄링을 제공한다. `schedule.every().day.at("10:30").do(collect_data)`로 매일 특정 시간에 실행하거나, `schedule.every(6).hours.do(collect_data)`로 6시간마다 실행할 수 있다. Linux/Mac에서는 crontab을 사용할 수 있고, Windows에서는 Task Scheduler를 사용한다. 중요한 것은 가상환경의 Python 경로를 정확히 지정해야 한다는 점이다.

1년치 히스토리컬 데이터를 효율적으로 수집하는 전략은 **Rate Limiting과 배치 처리**를 결합하는 것이다. ratelimit 라이브러리의 `@limits(calls=30, period=60)` 데코레이터로 분당 30회로 제한하고, 지수 백오프(exponential backoff)로 429 에러 시 재시도 간격을 점진적으로 늘린다. 365일 데이터를 한 번에 요청하는 대신, 월 단위로 12번 요청하거나 주 단위로 52번 요청하여 API 부하를 분산시킨다. 네이버 API는 월 단위 집계를 지원하므로, 장기 트렌드 분석에는 월 단위가 효율적이다.

## 한국 특화 데이터 소스와 크롤링

네이버 검색광고 키워드 도구 API는 무료 광고 계정만 있으면 접근 가능한 강력한 리소스다. searchad.naver.com에서 가입 후 manage.searchad.naver.com의 도구 섹션에서 API 자격증명을 생성한다. **월간 PC/모바일 검색량, 경쟁 지수, 클릭률, 그리고 입찰가 제안**을 제공하여 네이버 데이터랩의 상대 비율 데이터를 보완한다. PyPI의 naver-search-ad 패키지로 간단히 통합할 수 있으며, 관련 키워드 검색, 캠페인 관리, 성과 리포트를 API로 처리할 수 있다.

카카오 데이터 접근은 제한적이다. 카카오 모멘트 광고 플랫폼 API는 비즈니스 인증과 카카오의 공식 승인이 필요하며, 일반 개발자가 접근하기 어렵다. 카카오 트렌드 데이터는 공개 API가 없고, 웹 인터페이스도 제한적이다. 현실적인 대안은 카카오 로그인 API를 통해 사용자 데이터에 접근하거나, 공개 카카오 채널의 콘텐츠를 모니터링하는 것이다. 예산 제약을 고려하면 **네이버 에코시스템에 집중하는 것이 더 효과적**이다.

F&B 프랜차이즈 이벤트 크롤링은 법적으로 허용 가능하다. **한국 대법원 판례(2021도1533, 2022년 5월)는 공개된 데이터의 스크래핑을 합법으로 판결**했으며, 조건은 (1) 공개 접근 가능, (2) 기술적 차단 없음, (3) 약관에 명시적 금지 없음, (4) 접근 제한 우회 없음, (5) 데이터 보유자 이익 침해 없음이다. 실무적으로는 robots.txt 확인, 합리적인 Rate Limiting(요청 간 1-2초 대기), 명확한 User-Agent 식별, 그리고 개인정보 수집 금지를 준수해야 한다.

주요 타겟은 스타벅스 코리아, 이디야, 투썸플레이스, 할리스 커피 등이다. BeautifulSoup으로 정적 콘텐츠를 파싱하고, Selenium으로 JavaScript로 로드되는 동적 콘텐츠를 처리한다. 이벤트 페이지는 일반적으로 `/promotion`, `/event`, `/notice` 경로에 있으며, 리스트 페이지에서 상세 페이지로 링크된다. HTML 구조는 `<div class="event-item">`같은 패턴을 따르므로, 브라우저 개발자 도구(F12)로 인스펙트하여 정확한 선택자를 찾는다. 크롤링 스크립트에는 에러 핸들링, 재시도 로직, 그리고 수집 날짜 타임스탬프를 포함해야 한다.

## 통합 시스템 아키텍처와 구현 로드맵

권장 아키텍처는 **모놀리식 ELT 패턴**이다. 소규모 프로젝트에는 마이크로서비스가 과도하게 복잡하며, 단일 애플리케이션이 개발 속도와 디버깅에서 우수하다. 데이터 소스(네이버 API, 공휴일, 웹 크롤링) → 추출(Python requests) → 저장(DuckDB) → 변환(Pandas) → 오케스트레이션(schedule) → 시각화(Streamlit) 순서로 구성된다. **DuckDB는 SQLite보다 분석 쿼리에서 100배 빠르며** OLAP에 최적화되어 있어, 트렌드 데이터 집계에 완벽하다.

프로젝트 구조는 Cookiecutter Data Science 표준을 따른다. 루트에 requirements.txt, .env, README.md를 배치하고, `config/` 디렉토리에 설정 파일, `data/`에 raw/processed 서브폴더, `src/`에 data/etl/dashboard 모듈, `scripts/`에 실행 스크립트를 구성한다. 환경 변수는 python-decouple로 관리하고, API 키는 절대 코드에 하드코딩하지 않는다. Pydantic으로 설정을 검증하고, loguru로 구조화된 로깅을 구현한다.

MVP 로드맵은 10-14일로 구성된다. 1-2일차에 프로젝트 구조와 환경 설정, 3-4일차에 네이버 API 클라이언트와 캐싱 구현, 5-6일차에 DuckDB 스키마와 기본 ETL, 7-9일차에 Streamlit 대시보드와 첫 번째 차트, 10일차에 통합 테스트와 버그 수정을 완료한다. 성공 기준은 하나의 API에서 데이터를 가져와 데이터베이스에 저장하고 대시보드에 표시하는 것이다. 이후 3-4주차에 모든 API 통합과 고급 기능을 추가하고, 5-6주차에 프로덕션 준비(에러 핸들링, 모니터링, 문서화)를 완료한다.

핵심 의존성은 최소화한다. `duckdb`, `pandas`, `requests`, `dash`, `plotly-calplot`, `holidays`, `PyNaver`, `google-generativeai`, `python-decouple`, `schedule`, `loguru` 정도면 충분하다. 전체 설치 크기는 약 200MB로, Streamlit Community Cloud의 제한 내에 여유 있게 들어간다. Docker 컨테이너화는 선택 사항이지만, 재현 가능한 환경을 위해 권장된다. `FROM python:3.11-slim` 베이스 이미지에 requirements를 복사하고, 포트 8050을 노출하며, 대시보드 실행 스크립트를 CMD로 지정하는 간단한 Dockerfile로 충분하다.

## 비용 분석과 최종 권장사항

**전체 시스템을 월 0원에 운영할 수 있다.** 네이버 데이터랩 API(무료), Google Gemini API(무료), Streamlit Community Cloud(무료), GitHub Actions(퍼블릭 저장소 무료), DuckDB(로컬, 무료), Python 라이브러리들(오픈소스)이 모두 비용이 없다. 유일한 잠재적 비용은 프라이빗 저장소 사용 시 GitHub Actions 시간 초과나, Streamlit 1GB RAM 초과 시 발생할 수 있지만, 검색 트렌드 대시보드는 이 한도 내에 충분히 수용된다.

월 1,000개 키워드 확장 요청(약 550K 토큰)을 처리할 때 비용 시뮬레이션은 다음과 같다. Gemini 무료: 0원, Ollama 로컬: 0원, Claude API: 약 2,200원, OpenAI GPT-4o-mini: 약 1,850원이다. 모든 옵션이 2만원 예산 내에 있지만, **Gemini 무료 티어와 네이버 데이터랩으로만 구성하면 100% 무료로 프로덕션급 시스템을 구축**할 수 있다. 추가 예산이 있다면 SerpApi(월 9달러, 약 12,000원)를 고려하여 Google Trends 데이터를 안정적으로 가져올 수 있다.

구현 우선순위는 명확하다. 1단계로 네이버 데이터랩 API 통합과 DuckDB 저장, 2단계로 Streamlit 기본 대시보드와 히트맵, 3단계로 Gemini 키워드 확장 추가, 4단계로 공휴일 데이터 표시, 5단계로 F&B 크롤링(선택 사항)을 진행한다. 각 단계는 독립적으로 가치를 제공하며, MVP는 1-2단계만으로도 충분히 유용하다. 전체 시스템은 중급 Python 개발자가 4-6주 내에 완성할 수 있으며, 유지보수 시간은 주당 1-2시간 정도면 충분하다.

기술 스택 선택의 핵심은 **무료, 한국 특화, 유지보수 가능**이라는 세 가지 원칙이다. 네이버 데이터랩은 한국 시장에서 Google Trends보다 정확하고, Gemini는 한국어 지원이 뛰어나며, Streamlit은 배포가 쉽고, DuckDB는 분석 성능이 우수하다. 이 조합은 2025년 현재 시점에서 가장 최신이고 실전 검증된 기술들이며, 각각 활발한 커뮤니티와 풍부한 문서를 가지고 있어 문제 해결이 용이하다. 프로젝트를 시작하려면 developers.naver.com에서 API 키를 발급받고, GitHub 저장소를 생성하고, 제시된 폴더 구조를 따라 첫 번째 API 호출을 구현하는 것부터 시작하면 된다.